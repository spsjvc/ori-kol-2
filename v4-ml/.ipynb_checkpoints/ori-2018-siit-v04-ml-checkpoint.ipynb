{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Osnovi računarske inteligencije\n",
    "\n",
    "## Vežba 4 - Mašinsko učenje\n",
    "\n",
    "---\n",
    "\n",
    "Umesto priče:\n",
    "\n",
    "![img/ml_one_slide.jpg](img/ml_one_slide.jpg)\n",
    "\n",
    "Dva najčešće korišćena tipa mašinskog učenja su:\n",
    "* Nadgledano učenje (*eng. supervised learning*)\n",
    "* Nenadgledano učenje (*eng. unsupervised learning*)\n",
    "\n",
    "---\n",
    "\n",
    "## Nadgledano učenje\n",
    "\n",
    "Učenje funkcije na osnovu obeleženih podataka, odnosno predviđanje *y* u odnosu na *x* iz primera *(x,y)*. Performanse postupka se mere određenom metrikom/greškom. Veliki broj postupaka, neki od najpopularnijih su:\n",
    "* Regresiona analiza\n",
    "* Veštačke neuronske mreže\n",
    "* Stabla odlučivanja + Random forests\n",
    "* Naivni Bayes klasifikator\n",
    "* kNN (k nearest neighbors)\n",
    "* SVM (support vector machine)\n",
    "* ... i još mnogo drugih ...\n",
    "\n",
    "---\n",
    "\n",
    "## Nenadgledano učenje\n",
    "\n",
    "Učenje funkcije koja opisuje \"skrivenu\" strukturu neobeleženih podataka, odnosno učenje reprezentacije podataka na osnovu primera *x* (nema *y*). Nenadgledano učenje može biti korisno za pronalaženje interesantnih veza među podacima. Ne postoji neki standardni način merenja performansi. Postupci se dele na:\n",
    "* Klasterizacija/klasterovanje (*eng. clustering*) - grupisanje podataka na osnovu sličnosti\n",
    "* Analiza komponenti (*eng. component analysis*) - otkrivanje najdeskriptivnikih osobina podataka\n",
    "* Asocijativna pravila (*eng. association rules*) - pronalaženje uobičajenih kombinacija osobina podataka\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "### Jednostavna linearna regresija\n",
    "\n",
    "Jednostavna linearna regresija je statistički metod koji omogućava proučavanje veze između dve\n",
    "kontinualne promenljive.\n",
    "\n",
    "Prva promenljiva, **x**, se naziva prediktor ili **nezavisna promeljiva**.\n",
    "\n",
    "Druga promenljiva, **y** se naziva odziv ili **zavisna promenljiva**.\n",
    "\n",
    "Jednostavna linearna regresija ima pridev \"jednostavna\" samo zato što ima samo jednu nezavisnu promenljivu (x).\n",
    "Postoji i višestruka linearna regresija, koja se odnosni na linearnu regresiju sa više nezavisnih promenljivih.\n",
    "\n",
    "Pre opisa same linearne regresije, važno je napomenuti kakve veze/zavisnosti između promenljivih su od interesa.\n",
    "\n",
    "---\n",
    "\n",
    "**Deterministička (funkcionalna) zavisnost** nam nije od interesa jer njom možemo odrediti **tačnu** vrednost zavisne promenljive (y) na osnovu vrednosti nezavise promenljive (x). Primer ovakve veze je, recimo, konvertovanje mernih\n",
    "jedinica temperature između stepeni Celzijusa i Farenhajta:\n",
    "\n",
    "$$\n",
    "\\text{Fahr} = \\frac{9}{5} * \\text{Cels} + 32\n",
    "$$\n",
    "\n",
    "![](img/celcius_fahr_plot.gif)\n",
    "\n",
    "Znajući temperaturu u stepenima Celzijusa, možemo iskoristiti prethodnu jednačinu da **tačno** odredimo temperaturu u Farenhajtima. Još neki primeri determinističke zavisnosti su:\n",
    "\n",
    "* $ \\text{obim} = 2 \\pi * \\text{poluprecnik} $\n",
    "* Omov zakon: $ \\text{I} = \\frac{V}{R} $\n",
    "* Hukov zakon: $ \\text{F} = -\\text{k}\\text{x} $\n",
    "\n",
    "---\n",
    "\n",
    "Za svaku od ovih determinističkih zavisnosti, jednačina tačno opisuje odnos dveju promenljivih. Samim tim, ovaj kurs se ne bavi determinističkim vezama. Ono što će nam biti od interesa je ** statistička zavisnost **, gde veza između dve promenljive nije savršeno tačna.\n",
    "\n",
    "Primer statističke zavisnosti je određivanje stope smrtnosti od raka kože (broj smrti na 10 miliona ljudi) na osnovu geografske širine centra svake od 49 država u SAD (**data/skincancer.csv**)\n",
    "\n",
    "Može se pretpostaviti da u zemljama koje su severnije, ljudi su manje izloženi sunčevoj svetlosti i štetnim uticajima sunčevih zraka (UV zračenja) i samim tim bi rizik smrti od raka kože trebao biti manji. Dole prikazani grafik podržava takvu hipotezu - vidi se negativna linerna zavisnost između geografske širine i stope smrtnosti od raka kože, ali ova veza nije savršeno linearna. Dakle, ovo je statistička zavisnost, a ne deterministička.\n",
    "\n",
    "![](img/scatterplot_skin_cancer.png)\n",
    "\n",
    "Još neki primeri statističke zavisnosti mogu biti:\n",
    "\n",
    "* Visina i težina ljudi - uglavnom očekujemo da su više osobe teže, ali ne mora nužno biti tako\n",
    "* Kapacitet pluća i količina konzumiranih cigareta tokom života - kako se količina konzumiranih cigara povećava, očekuje se smanjenje kapaciteta pluća\n",
    "* Broj ljudi u prostoriji i temperatura/vlažnost prostorije\n",
    "* Količina uloženog truda tokom studiranja (kako ovo izmeriti?) i prolaznost / prosek ocena\n",
    "\n",
    "---\n",
    "\n",
    "##### Koja je \"najpogodnija linija\"?\n",
    "\n",
    "Pošto je neophodno pronaći neku linearu zavisnost između dve promenljive, nameće se pitanje koja je to najpogodnija linija koji opisuje tu zavisnost? U principu, zanima nas da pronađemo liniju $ \\hat{y} = a + bx $ koja na najbolji način opisuje date podatke. Podaci su dati kao:\n",
    "\n",
    "* $ x_i $ - vrednost nezavisne promenljive za svako merenje *i*\n",
    "* $ y_i $ - vrednost zavisne promenljive za svako merenje *i*\n",
    "* $ \\hat{y}_i $ - vrednost *predviđene* zavisne promenljive za svako merenje *i*\n",
    "\n",
    "Očigledno je da će se $ y_i $ i $ \\hat{y}_i $ razlikovati u određenoj meri (u nekim merenjima manje u nekim merenjima više), ali je potrebno *u proseku* smanjiti ovu razliku. Jednostavna linearna regresija je postupak koji pronalazi najpogodniju liniju koja opisuje zadate podatke $ x_i $ i $ y_i $, minimizujući grešku predikcije $ e_i = y_i - \\hat{y}_i $.\n",
    "\n",
    "---\n",
    "\n",
    "##### Izvođenje linearne regresije\n",
    "\n",
    "Linija koja najbolje \"fituje\" (od eng. *fit* - pristajati) podatke će biti ona linija za koje su **n grešaka predikcije** - za svako merenje - **male koliko god je moguće u nekom opštem smislu**. Jedan način za postizanje ovog cilja je **kriterijum najmanjih kvadrata**, koja kaže da se treba *minimizovati suma kvadrata grešaka predikcije*. Odnosno:\n",
    "\n",
    "* Jednačina linije koja najbolje \"fituje\" je $ \\hat{y} = a + bx $ - *a* je odsečak na y-osi, a *b* je nagib linije\n",
    "* Potrebno je odrediti vrednosti *a* i *b* koje će činiti da je suma kvadratnih grešaka predikcije što je moguće manja\n",
    "* Dakle, potrebno je pronaći *a* i *b* koji minimizuju: $$ Q=\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2 = \\sum_{i=1}^{n}(y_i-a-bx_i)^2 $$\n",
    "\n",
    "\n",
    "Kako bismo pronašli *a* i *b* koje minimizuju *Q*, pronaći ćemo parcijalne izvode (seća li se iko?) od *Q* u odnosu na *a* i *b* i izjednačiti ih sa nulom, i rešiti ih za *a* i *b*:\n",
    "$$\n",
    "\\frac{\\partial Q}{\\partial a} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Q}{\\partial b} = 0\n",
    "$$\n",
    "\n",
    "Rešenje:\n",
    "$$\n",
    "a = \\frac{1}{n}\\sum_{i=1}^{n}(y_i-bx_i) = \\bar{y} - b\\bar{x}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = \\frac{\\frac{1}{n}\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\frac{1}{n}\\sum_{i=1}^{n}(x_i-\\bar{x})^2}=\\frac{Cov(x,y)}{Var(x)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Višestruka linearna regresija\n",
    "\n",
    "Za razliku od jednostavne linearne regresije koja koristi samo jednu prediktorsku promenljivu *x*, višestruka linearna regresija podrazumeva korišćenje dve ili više prediktorskih promenljivih. Na prethodnom primeru, mogli bismo dodati: kako zavisi stopa smrtnosti od raka kože u odnosu na geografsku širinu države **i** cene kreme za sunčanje, prosečne plate, klime?\n",
    "\n",
    "Višestruka linearna regresija je odličan alat za otkrivanje interesantnih zavisnosti između podataka, koje možda nisu nisu očigledne na prvi pogled.\n",
    "\n",
    "U opštem obliku, višestruka linearna regresija podrazumeva jednačinu:\n",
    "$ \\hat{y} = a + b_1x_1 + b_2x_2 + ... + b_px_p $, gde je neophodno pronaći parametre *a* i *b* na isti način kao kod jednostavne linearne regresije - minimizacijom sume kvadratnih grešaka predikcije $ Q=\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2 $.\n",
    "\n",
    "---\n",
    "\n",
    "### Zadaci\n",
    "\n",
    "**TODO 1**: Implementirati jednostavu linearnu regresiju. Ulazi parametri su liste/vektori *x* i *y*, koji predstavljaju podatke, a povratne vrednosti su *slope* i *intercept* koji predstavlju nagib i presečnu tačku linije koja najbolje \"fituje\" podatke iz ulaznih parametara. Fajl ```src/linreg_simple.py``` -> funkcija ```linear_regression(x, y)```.\n",
    "\n",
    "**TODO 2**: Implementirati predviđanje vrednosti *y* na osnovu date vrednosti za *x* i parametara linije *slope* i *intercept*. Fajl ```src/linreg_simple.py``` -> funkcija ```predict(x, slope, intercept)```.\n",
    "\n",
    "**TODO 3**: Izvršiti linearnu regresiju na primeru predviđanja stope smrtnosti od raka kože na osnovu geografske širine američkih država.\n",
    "* Učitati datoteku *data/skincancer.csv* i implementirati primenu linearne regresije u Python fajlu ```src/skin_cancer.py```.\n",
    "* Koristeći biblioteku <a href=\"https://matplotlib.org/\"><b>Matplotlib</b></a> iscrati grafik za ovaj slučaj.\n",
    "* Izvršiti predikciju stope smrtnosti za državu Hawaii (*Lat=21.3, Long=157.9*).\n",
    "* Probati linearnu regresiju nad ovim istim skupom podataka, ali da se umesto geografske širine koristi geografska dužina.\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "### K-means\n",
    "\n",
    "Jedan od najčešće korišćenih algoritama za nenadgledano klasterovanje podataka. Preciznije, k-means je ne-hijerarhijska metoda grupisanja sličnih podataka. K-means je tehnika koja se često koristi u tzv. *eksplorativnoj analizi podataka*.\n",
    "\n",
    "Klaterizacija je zadatak grupisanja skupa objekata, tako da su objekti koji su u istoj grupi (odnosno *klasteru*) sličniji (u nekom smislu) jedni drugima, više nego što su slični objektima u drugim grupama (klasterima).\n",
    "\n",
    "Pseudo-kod:\n",
    "\n",
    "```\n",
    "za svaku grupu inicijalizovati nasumično centar\n",
    "dok se centri ne prestanu kretati ili ne dostigne max broj iteracija:\n",
    "    pridruži svaki element grupi sa njemu najbližim centrom grupe\n",
    "    pomeri centar svih grupa na osnovu novih elemenata\n",
    "```\n",
    "\n",
    "![img/kmeans.gif](img/kmeans.gif)\n",
    "\n",
    "\n",
    "\n",
    "#### Određivanje optimalnog K\n",
    "\n",
    "Kako znati unapred koliko ima klastera? Lako je videti kada su podaci dvodimenzionalni, jer ih je onda lako i vizualizovati, ali često podaci imaju (mnogo) više od samo 2 dimenzije - ovo je tzv. \"kletva dimenzionalnosti\" (*eng. curse of dimensionality*) u mašinskom učenju.\n",
    "\n",
    "Određivanje optimalnog K (tj. broja klastera) je nešto se dosta proučavalo, a mi ćemo koristi tzv. \"metodu lakta\" (*eng. elbow method*). Za određen broj K (npr. 2, 4, 6, 8, ..., 20) se vrši klasterizacija i zatim se računa suma kvadratnih grešaka (SSE). SSE se računa tako što se unutar svakog klastera sumiraju kvadrati udaljenosti podataka od centra klastera, i zatim se sve to opet sumira. Matematički:\n",
    "\n",
    "$ SSE = \\sum_{i=1}^{K} \\sum_{x \\in c_{i}} dist(x, c_{i})^{2} $, gde je *dist* euklidska udaljnost.\n",
    "\n",
    "Zatim se za sve plotuje SSE u odnosu na K, npr.:\n",
    "\n",
    "![img/sse.png](img/sse.png)\n",
    "\n",
    "---\n",
    "\n",
    "#### Prednosti K-means\n",
    "\n",
    "* Jednostavan i lako razumljiv\n",
    "* Laka implementacija\n",
    "* Relativno dobre performanse (za malo K)\n",
    "* Odličan kada su klasteri sferičnog/globularnog oblika (malo formalnije hiper-sferičnog, za sfere u >3 dimenzija)\n",
    "\n",
    "#### Mane K-means\n",
    "* Potrebno unapred znati K (što je nekad teško odrediti)\n",
    "* Nije deterministički - pošto se centri inicijalizuju nasumično, nekad se dobijaju drugačiji rezultati\n",
    "* Osetljiv na šum\n",
    "* Kada podaci nisu globularnog oblika -> beskoristan (pogledati donju sliku)\n",
    "* Nema mogućnost hijerarhijskog klasterovanja (razlikovanje više manjih podklastera unutar većeg klastera)\n",
    "\n",
    "![img/kmeans_fail.png](img/kmeans_fail.png)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### DBSCAN\n",
    "\n",
    "DBSCAN (Density-based spatial clustering of applications with noise) je takođe algoritam za klasterizaciju podataka. Ovaj postupak se zasniva na ideji grupisanja tačaka (podataka) na osnovu njihove međusobne udaljenosti. Ukoliko se tačke nalaze u tzv. *epsilon okolini* one su deo nekog klastera, u suprotnom se posmatraju kao šum.\n",
    "\n",
    "Iako predstavljen još 1996. godine, 2014. godine na jednoj od najprestižnijih konferencija (KDD), DBSCAN je nagrađen nagradom \"testa vremena\", kao jedan od najviše citiranih algoritama sa ogromnom upotrebom, kako u teoriji tako i u praksi.\n",
    "\n",
    "\n",
    "Opis DBSCAN algoritma:\n",
    "1. Neka postoji neki skup tačaka (podataka) koje želimo da klasterizujemo. U samom postupku, razlikuju se tri vrste tačaka: ključne tačke, dostupne tačke i šum.\n",
    "2. DBSCAN zahteva dva parametra: *epsilon* (eps) i *minimalni broj potrebnih tačaka koje čine region* (minPts). Epsilon okolina se najčešće računa korišćenjem euklidske udaljenosti.\n",
    "3. Algoritam počinje sa proizvoljnom tačkom. Računa se epsilon okolina te tačke, i ukoliko se u njoj nalazi dovoljno tačaka (minPts), započinje se novi klaster. U suprotnom, tačka se računa kao šum. Obratiti pažnju da tačka, iako je šum, kasnije *može* biti pronađena kao deo neke druge epsilon okoline sa dovoljno tačaka i samim tim da postane deo klastera.\n",
    "4. Ukoliko je za tačku određeno da pripada klasteru, sve tačke u njenoj epsilon okolini takođe pripadaju tom klasteru. Dakle, sve tačke koje su pronađene u epsilon okolini trenutne tačke se dodaju u klaster, kao i tačke koje se nalaze u epsilon okolini tih tačaka (rekurzivno). Proces se nastavlja dok se ne nađe ceo klaster, odnosno dok se ne obiđu sve tačke u epsilon okolinama. \n",
    "5. Onda se nalazi nova, prozivoljna neposećena tačka, za koju se ponavlja čitav postupak, što dovodi do otkrivanja ili novog klastera ili šuma.\n",
    "\n",
    "![img/dbscan.png](img/dbscan.png)\n",
    "\n",
    "---\n",
    "\n",
    "#### Prednosti DBSCAN\n",
    "\n",
    "* Nije potrebno unapred znati broj klastera (kao kod K-means)\n",
    "* Klasteri mogu biti proizvoljnog oblika\n",
    "* Ume da tretira šum\n",
    "* Parametre epsilon i minPts je lako menjati u cilju dobijanja klastera različitih veličina i oblika, i ove parametre često podešavaju eksperti sa domenskim znanjem\n",
    "\n",
    "\n",
    "#### Mane DBSCAN\n",
    "\n",
    "* Kvalitet rezultata zavisi od toga čime se meri epsilon. Obično je to euklidska udaljenost, ali za višedimenzionalne podatke potrebne su drugačije metrike\n",
    "* Kada postoje varijacije u gustini klastera, nemoguće je odrediti epsilon i minPts da odgovara svim klasterima\n",
    "* U slučaju kada ne postoji ekspert sa domenskim znanjem, određivanje epsilon i minPts parametara je često dosta teško\n",
    "\n",
    "---\n",
    "\n",
    "### Zadaci\n",
    "\n",
    "** TODO 1: ** Implementirati K-means algoritam (u ```src/kmeans.py``` sve što je označeno sa TODO 1):\n",
    "  * Metodu ```fit``` klase ```KMeans```, koja kao parametar prima n-dimenzionalne podatke koje treba klasterizovati.\n",
    "  * Metodu ```predict``` klase ```KMeans```, koja kao parametar prima jedan podatak i vraća indeks klastera kojem pripada (na osnovu euklidske udaljenosti).\n",
    "  * Metodu ```recalculate_center``` klase ```Cluster```, koja treba da izračuna centar klastera kao srednju vrednost svih podataka u njemu i da vrati ovu vrednost kao rezultat.\n",
    "  * ** Za domaći ** - proširiti metodu ```fit``` tako da se algoritam zaustavi kada se centri klastera više ne pomeraju.\n",
    "  \n",
    "** ТОDO 2: ** Primeniti K-means algoritam na Iris data setu (https://en.wikipedia.org/wiki/Iris_flower_data_set), u cilju pronalaženja dva klastera u odnosu na širinu sepala i dužinu petala. Pokrenuti skriptu ```src/iris.py```.\n",
    "\n",
    "![img/iris.png](img/iris.png)\n",
    "\n",
    "** TODO 3: ** Implementirati izračunavanje sume kvadratnih grešaka (SSE) u metodi ```sum_squared_error``` klase ```KMeans```. Zatim ponovo pokrenuti ```src/iris.py``` skriptu i odrediti optimalno K na osnovu grafika.\n",
    "\n",
    "** TODO 4: ** Normalizovati podatke pre primene K-means algoritma. Normalizacija je potrebna kada su opsezi vrednosti u podacima različiti (npr. ako se visina ljudi meri u metrima, a težina u gramima). Normalizacija se može vršiti na više načina, dva najčešća su:\n",
    "* Svođenje podataka na isti interval, npr. [-1, 1]. Šta je neophodno znati o podacima za ovakvu normalizaciju?\n",
    "* Centriranje i normalizacija standardnom devijacijom -> x = (x - srednja_vrednost(x)) / standardna_devijacija(x). Ovakva normalizacije se najčešće koristi i nije karakteristična samo za K-means, već generalno za analizu podataka u mašinskom učenju.\n",
    "\n",
    "** TODO 5: ** Primeniti K-means na podatke generisane u ```src/ball_circle.py```.\n",
    "\n",
    "### Domaći\n",
    "\n",
    "** TODO 6: ** Implementirati DBSCAN algoritam u metodi ```fit``` klase ```DBSCAN```, ```src/dbscan.py```.\n",
    "\n",
    "** TOOD 7: ** Primeniti DBSCAN nad Iris podacima ```src/iris.py``` i nad podacima u skripti ```src/ball_circle.py```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
